#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fatigue Edge RPi — Picamera2-only + MediaPipe Tasks-only + CSV + UI EN HILO PRINCIPAL
Hilos: T0Capture, T2FaceMesh, T3Features, T4Decision (MQTT+CSV)
UI (imshow) corre en el hilo principal.

Arreglos clave:
- PERCLOS basado en estado de ojo cerrado (booleano con histéresis) — no en el EAR en crudo
- Blinks con FSM por duración (min/max frames) + refractario — ya no se pierden ni se duplican
- EAR adaptativo por persona sólo para fijar umbral; histéresis para estabilidad
- Yawn robusto (histeresis + duración) para no confundir hablar con bostezo
"""

import time, json, threading, queue, argparse, os, ssl
from collections import deque
from dataclasses import dataclass
from typing import Optional
import numpy as np
import cv2
import csv

# ====== MQTT (AWS IoT Core) ======
try:
    from paho.mqtt import client as mqtt
except Exception:
    mqtt = None

# ====== Picamera2 (OBLIGATORIO) ======
from picamera2 import Picamera2

# ====== MediaPipe Tasks (OBLIGATORIO) ======
import mediapipe as mp
from mediapipe.tasks import python as mp_python
from mediapipe.tasks.python import vision

# ---------------- Config ----------------
CAM_SIZE = (640, 480)
TARGET_FPS = 30
FACEMESH_HZ = 24.0  # landmarks a ~24 Hz

# ventana PERCLOS (más reactiva)
PERCLOS_WINDOW_SEC  = 6.0

# tasas por minuto (ventana rolling)
EVENT_RATE_WINDOW_S = 60.0
MIN_COVERAGE_BLINKS_S = 45.0
MIN_COVERAGE_YAWNS_S  = 30.0

PUBLISH_HZ = 1.0

# --------- AWS IoT Core ----------
ENDPOINT   = "a1omfl67425kjv-ats.iot.us-east-2.amazonaws.com"
PORT       = 8883
CLIENT_ID  = "rsp5"
TOPIC      = "alertas"

CA_PATH   = "aws/AmazonRootCA1.pem"
CERT_PATH = "aws/certificate.pem.crt"
KEY_PATH  = "aws/private.pem.key"

MODEL_PATH = "facemesh/face_landmarker.task"

CSV_FILENAME = "session_log.csv"
WINDOW_NAME  = "Fatigue Monitor RPi"

# --------- Umbrales robustos ---------
# EAR adaptativo: th_close = max(EAR_MIN_TH, EAR_FRAC_TH * mediana últimos ADAPT_EAR_WIN_SEC)
ADAPT_EAR_WIN_SEC = 3.0
EAR_MIN_TH = 0.17
EAR_FRAC_TH = 0.72
HYST_MARGIN = 0.03  # th_open = th_close + 0.03

# Blink FSM (duración) en frames @ FACEMESH_HZ
BLINK_MIN_FRAMES = int(0.08 * FACEMESH_HZ)  # ~80 ms
BLINK_MAX_FRAMES = int(0.40 * FACEMESH_HZ)  # ~400 ms
BLINK_REFRACT_FR = int(0.25 * FACEMESH_HZ)  # ~250 ms refractario

# Yawn robusto (histeresis + duración)
MAR_YAWN_ON  = 0.78
MAR_YAWN_OFF = 0.62
MIN_YAWN_FR  = int(0.90 * FACEMESH_HZ)  # ~0.9 s sostenido

# EMA suavizado ligero (sólo para señal, NO para FSM)
EMA_ALPHA = 0.35

# ---------------- Data ----------------
def now() -> float: return time.time()

@dataclass
class Frame: ts:float; bgr:np.ndarray; fps:float

@dataclass
class Landmarks: ts:float; pts:np.ndarray  # (468,2)

@dataclass
class Features:
    ts:float; ear:float; mar:float; perclos:float
    blinks:int; yawns:int
    blink_cov_s:float; yawn_cov_s:float

# ---------------- Queues ----------------
stop = threading.Event()
q_frame = queue.Queue(maxsize=4)
q_lm    = queue.Queue(maxsize=2)
q_feat  = queue.Queue(maxsize=4)

def put_drop(q:queue.Queue, item):
    try: q.put(item, timeout=0.003)
    except queue.Full:
        try: q.get_nowait()
        except Exception: pass
        q.put(item)

def get_latest(q:queue.Queue):
    last = None
    try:
        while True: last = q.get_nowait()
    except queue.Empty: pass
    return last

# ---------------- Estado compartido ----------------
class Shared:
    def __init__(self):
        self.lock = threading.Lock()
        self.last_frame = None   # (bgr, ts, fps)
        self.last_feat  = None   # Features
        self.face_ok_ts = 0.0
shared = Shared()

# ---------------- MediaPipe FaceMesh (Tasks-only, VIDEO mode) ----------------
class MPFaceMesh:
    def __init__(self):
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"[FM] Modelo no encontrado: {MODEL_PATH}")
        base = mp_python.BaseOptions(model_asset_path=MODEL_PATH)
        opts = vision.FaceLandmarkerOptions(
            base_options=base,
            running_mode=vision.RunningMode.VIDEO,
            num_faces=1,
            min_face_detection_confidence=0.5,
            min_face_presence_confidence=0.5,
            min_tracking_confidence=0.5,
            output_face_blendshapes=False,
            output_facial_transformation_matrixes=False
        )
        self.detector = vision.FaceLandmarker.create_from_options(opts)
        print("[FM] MediaPipe Tasks activo (.task, VIDEO mode)")

    def __call__(self, bgr: np.ndarray, ts_ms:int) -> Optional[np.ndarray]:
        h, w = bgr.shape[:2]
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)
        r = self.detector.detect_for_video(mp_image, ts_ms)
        if not r.face_landmarks: return None
        lm = r.face_landmarks[0]
        pts = np.array([(p.x*w, p.y*h) for p in lm], dtype=np.float32)
        return pts

# ---------------- EAR / MAR ----------------
LE = [33,160,158,133,153,144]
RE = [263,387,385,362,380,373]
MOUTH_H = (13,14); MOUTH_W = (78,308)

def _ear6(p):
    d = lambda a,b: np.linalg.norm(p[a]-p[b])
    num = d(1,5)+d(2,4); den = 2.0*d(0,3)
    return (num/den) if den>1e-6 else 0.0

def compute_EAR(pts): return 0.5*(_ear6(pts[LE])+_ear6(pts[RE]))

def compute_MAR(pts):
    a,b = MOUTH_H; c,d = MOUTH_W
    h = np.linalg.norm(pts[a]-pts[b]); w = np.linalg.norm(pts[c]-pts[d])
    return (h/w) if w>1e-6 else 0.0

# ---------------- Filtros / Ventanas ----------------
class EMA:
    def __init__(self, a=EMA_ALPHA): self.a=a; self.y=None
    def __call__(self,x): self.y = x if self.y is None else self.a*x+(1-self.a)*self.y; return self.y

class AdaptiveEAR:
    """Umbral base por persona: max(EAR_MIN_TH, EAR_FRAC_TH * mediana últimos N s)."""
    def __init__(self, seconds, hz, frac=EAR_FRAC_TH, min_th=EAR_MIN_TH):
        self.buf = deque(maxlen=max(5, int(seconds*hz)))
        self.frac = frac
        self.min_th = min_th
        self.th = min_th
    def update(self, ear_value: float) -> float:
        self.buf.append(float(ear_value))
        if len(self.buf) >= 5:
            med = float(np.median(self.buf))
            self.th = max(self.min_th, self.frac * med)
        return self.th

class EyeHysteresis:
    """Convierte EAR en estado booleano cerrado/abierto con histéresis."""
    def __init__(self, margin=HYST_MARGIN):
        self.margin = margin
        self.closed = False
        self.last_th = EAR_MIN_TH
    def update(self, ear_val: float, th_close: float) -> bool:
        th_open = th_close + self.margin
        self.last_th = th_close
        if self.closed:
            if ear_val > th_open:
                self.closed = False
        else:
            if ear_val < th_close:
                self.closed = True
        return self.closed

class PERCLOSWindow:
    def __init__(self, seconds, hz): self.buf=deque(maxlen=int(seconds*hz))
    def update(self, eye_closed:bool)->float:
        self.buf.append(1 if eye_closed else 0)
        return sum(self.buf)/len(self.buf) if self.buf else 0.0

class EventRateWin:
    def __init__(self, win_s=60.0):
        self.win=win_s; self.ts=deque(); self.first=None
    def push(self,t):
        self.ts.append(t); self.first = self.first or t
        self._trim(t)
    def _trim(self,tn):
        while self.ts and (tn-self.ts[0]>self.win): self.ts.popleft()
        if self.first and (tn - self.first > self.win):
            self.first = tn - self.win
    def rate_and_cov(self, tn):
        self._trim(tn)
        rate = len(self.ts)
        cov  = 0.0 if self.first is None else max(0.0, min(self.win, tn - self.first))
        return rate, cov

class BlinkFSM:
    """Cuenta blinks por duración del cierre con refractario."""
    def __init__(self, min_f=BLINK_MIN_FRAMES, max_f=BLINK_MAX_FRAMES, refract_f=BLINK_REFRACT_FR):
        self.min_f=min_f; self.max_f=max_f; self.refract_f=refract_f
        self.closed_frames=0; self.refrac=0
        self.rate=EventRateWin(EVENT_RATE_WINDOW_S)
    def update(self, eye_closed: bool, t: float):
        if self.refrac>0: self.refrac -= 1
        if eye_closed:
            self.closed_frames += 1
        else:
            if self.closed_frames >= self.min_f and self.closed_frames <= self.max_f:
                if self.refrac == 0:
                    self.rate.push(t)
                    self.refrac = self.refract_f
            self.closed_frames = 0
    def measures(self,t): return self.rate.rate_and_cov(t)

class YawnDetector:
    """Histeresis + duración mínima para evitar hablar como bostezo."""
    def __init__(self, on_th=MAR_YAWN_ON, off_th=MAR_YAWN_OFF, min_len_frames=MIN_YAWN_FR):
        self.on=on_th; self.off=off_th
        self.min_len=min_len_frames
        self.opening=0
        self.rate=EventRateWin(EVENT_RATE_WINDOW_S)
        self.state=False
    def update(self, mar_s: float, t: float):
        if self.state:
            if mar_s < self.off:
                if self.opening >= self.min_len:
                    self.rate.push(t)
                self.opening = 0
                self.state = False
            else:
                self.opening += 1
        else:
            if mar_s > self.on:
                self.state = True
                self.opening = 1
    def measures(self,t): return self.rate.rate_and_cov(t)

# ---------------- Clasificador exacto ----------------
def closeness_to_range(x,a,b):
    if a<=x<=b: return 1.0
    d=min(abs(x-a),abs(x-b)); return max(0.0, 1.0 - d/max(1.0,(b-a)))

def classify_by_table(perclos, blinks, yawns, blink_conf, yawn_conf)->str:
    if perclos > 0.8:
        return "MICROSUEÑO"
    if blink_conf and blinks < 6:
        return "MICROSUEÑO"
    somno_blinks_ok = (6 <= blinks <= 12) if blink_conf else True
    somno_yawns_ok  = (yawns > 4) if yawn_conf else True
    if 0.4 <= perclos <= 0.8 and somno_blinks_ok and somno_yawns_ok:
        return "SOMNOLENCIA"
    fatiga_blinks_ok = (12 <= blinks <= 16) if blink_conf else True
    fatiga_yawns_ok  = (1 <= yawns <= 4) if yawn_conf else True
    if 0.3 <= perclos < 0.4 and fatiga_blinks_ok and fatiga_yawns_ok:
        return "FATIGA"
    normal_blinks_ok = (17 <= blinks <= 25) if blink_conf else True
    normal_yawns_ok  = (0 <= yawns <= 1) if yawn_conf else True
    if perclos < 0.3 and normal_blinks_ok and normal_yawns_ok:
        return "NORMAL"
    scores={
        "NORMAL"      : (0.3-perclos if perclos<0.3 else 0)+ (closeness_to_range(blinks,17,25) if blink_conf else 0.5) + (closeness_to_range(yawns,0,1) if yawn_conf else 0.5),
        "FATIGA"      : closeness_to_range(perclos,0.3,0.4) + (closeness_to_range(blinks,12,16) if blink_conf else 0.5) + (closeness_to_range(yawns,1,4) if yawn_conf else 0.5),
        "SOMNOLENCIA" : closeness_to_range(perclos,0.4,0.8) + (closeness_to_range(blinks,6,12)  if blink_conf else 0.5) + ((1.0 if yawn_conf and yawns>4 else 0.5))
    }
    if perclos>0.75:
        return "SOMNOLENCIA"
    return max(scores,key=scores.get)

# ---------------- MQTT (AWS IoT TLS) ----------------
def init_mqtt_aws(endpoint, port, client_id):
    if mqtt is None:
        print("[MQTT] paho no disponible — modo consola")
        return None
    c = mqtt.Client(client_id=client_id)
    c.tls_set(ca_certs=CA_PATH, certfile=CERT_PATH, keyfile=KEY_PATH, tls_version=ssl.PROTOCOL_TLSv1_2)
    c.connect(endpoint, port, keepalive=60)
    c.loop_start()
    print("[MQTT] AWS IoT conectado")
    return c

def mqtt_publish(client, topic, payload)->bool:
    if client is None:
        print(f"[MQTT-FAKE] {topic}: {json.dumps(payload)}")
        return True
    try:
        r = client.publish(topic, json.dumps(payload), qos=1); r.wait_for_publish(timeout=1.0)
        return r.is_published()
    except Exception:
        return False

# ---------------- Camera (Picamera2-only) ----------------
class CamPicam2:
    def __init__(self, size, fps):
        self.size=size; self.fps=fps; self.cam=None; self.last=now(); self._fps=0.0
    def start(self):
        self.cam = Picamera2()
        cfg = self.cam.create_video_configuration(main={"size": self.size, "format": "RGB888"})
        self.cam.configure(cfg); self.cam.start()
        print("[CAM] Picamera2 activo (OV5647)")
    def read(self):
        rgb = self.cam.capture_array()
        bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
        t = now(); self._fps = 1.0/max(1e-3, t-self.last); self.last=t
        return True, bgr, self._fps
    def stop(self):
        try: self.cam.stop()
        except Exception: pass

# ---------------- Threads ----------------
class T0Capture(threading.Thread):
    def __init__(self):
        super().__init__(daemon=True)
        self.cam=None; self.fps=0.0
    def run(self):
        self.cam = CamPicam2(CAM_SIZE, TARGET_FPS); self.cam.start()
        while not stop.is_set():
            ok, bgr, fps = self.cam.read()
            if not ok: continue
            self.fps=fps
            with shared.lock:
                shared.last_frame = (bgr.copy(), now(), fps)
            put_drop(q_frame, Frame(ts=now(), bgr=bgr, fps=fps))
        self.cam.stop()
    def get_fps(self): return self.fps

class T2FaceMesh(threading.Thread):
    def __init__(self):
        super().__init__(daemon=True); self.fm=MPFaceMesh()
        self.period=1.0/FACEMESH_HZ; self.last=0.0
    def run(self):
        while not stop.is_set():
            fr = get_latest(q_frame) or q_frame.get()
            if (now()-self.last) < self.period:
                time.sleep(0.001); continue
            self.last=now()
            ts_ms = int(fr.ts * 1000)
            pts = self.fm(fr.bgr, ts_ms)
            if pts is not None:
                with shared.lock:
                    shared.face_ok_ts = now()
                put_drop(q_lm, Landmarks(ts=fr.ts, pts=pts))

class T3Features(threading.Thread):
    def __init__(self):
        super().__init__(daemon=True)
        self.ema_e, self.ema_m = EMA(EMA_ALPHA), EMA(EMA_ALPHA)
        self.perclos = PERCLOSWindow(PERCLOS_WINDOW_SEC, FACEMESH_HZ)
        self.adapt_ear = AdaptiveEAR(ADAPT_EAR_WIN_SEC, FACEMESH_HZ)
        self.eye_hys = EyeHysteresis(HYST_MARGIN)
        self.blinks  = BlinkFSM(BLINK_MIN_FRAMES, BLINK_MAX_FRAMES, BLINK_REFRACT_FR)
        self.yawns   = YawnDetector(MAR_YAWN_ON, MAR_YAWN_OFF, MIN_YAWN_FR)
    def run(self):
        while not stop.is_set():
            lm = q_lm.get()
            # EAR/MAR "raw" + EMA sólo para suavizar ruido, no para FSM
            ear_raw = compute_EAR(lm.pts); mar_raw = compute_MAR(lm.pts)
            ear_s, mar_s = self.ema_e(ear_raw), self.ema_m(mar_raw)

            # Umbral adaptativo y estado de ojo con histéresis
            th_close = self.adapt_ear.update(ear_s)
            eye_closed = self.eye_hys.update(ear_s, th_close)

            # PERCLOS con el booleano (verdadero PERCLOS)
            p = self.perclos.update(eye_closed=eye_closed)

            # Blinks por duración de cierre + refractario
            self.blinks.update(eye_closed, lm.ts)

            # Yawn robusto (histeresis + duración)
            self.yawns.update(mar_s, lm.ts)

            bl, bl_cov = self.blinks.measures(lm.ts)
            yw, yw_cov = self.yawns.measures(lm.ts)

            with shared.lock:
                shared.last_feat = Features(ts=lm.ts, ear=ear_s, mar=mar_s, perclos=p,
                                            blinks=bl, yawns=yw,
                                            blink_cov_s=bl_cov, yawn_cov_s=yw_cov)
            put_drop(q_feat, shared.last_feat)

class T4Decision(threading.Thread):
    def __init__(self, client_id, mqtt_client, topic=TOPIC):
        super().__init__(daemon=True)
        self.did = client_id; self.mqtt=mqtt_client; self.topic=topic
        self.last_pub = 0.0
        # CSV
        self.csv_file = open(CSV_FILENAME, mode='w', newline='')
        self.csv_writer = csv.writer(self.csv_file)
        self.csv_writer.writerow(['ts','estado','perclos','blinks_min','yawns_min'])
    def run(self):
        try:
            while not stop.is_set():
                tnow = now()
                if (tnow - self.last_pub) < (1.0 / PUBLISH_HZ):
                    time.sleep(0.01); continue
                self.last_pub = tnow

                with shared.lock:
                    f = shared.last_feat

                if f is None:
                    ts_i = int(tnow)
                    self.csv_writer.writerow([ts_i, "SIN_DATOS", "NaN", "NaN", "NaN"])
                    self.csv_file.flush()
                    continue

                blink_conf = (f.blink_cov_s >= MIN_COVERAGE_BLINKS_S)
                yawn_conf  = (f.yawn_cov_s  >= MIN_COVERAGE_YAWNS_S)
                estado = classify_by_table(f.perclos, f.blinks, f.yawns, blink_conf, yawn_conf)

                payload = {
                    "device_id": self.did,
                    "estado": estado,
                    "perclos": round(float(f.perclos), 2),  # [0..1]
                    "blinks": int(f.blinks),                # eventos/min (últ 60s)
                    "yawns": int(f.yawns),                  # eventos/min (últ 60s)
                    "ts": int(f.ts)                         # epoch
                }
                mqtt_publish(self.mqtt, self.topic, payload)

                self.csv_writer.writerow([payload["ts"], payload["estado"], payload["perclos"],
                                          payload["blinks"], payload["yawns"]])
                self.csv_file.flush()
        finally:
            try: self.csv_file.close()
            except: pass

# ---------------- HUD ----------------
def draw_hud(bgr, feat:Optional[Features], estado:str, face_recent_s:float, fps:float):
    hud = bgr
    h, w = bgr.shape[:2]
    cv2.rectangle(hud, (0,0), (w, 34), (0,0,0), -1)

    color = (200,200,200)
    if estado=="NORMAL": color=(0,255,0)
    elif estado=="FATIGA": color=(0,255,255)
    elif estado=="SOMNOLENCIA": color=(0,165,255)
    elif estado=="MICROSUEÑO": color=(0,0,255)

    cv2.putText(hud, f"ESTADO: {estado}", (10,24), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)

    if feat:
        cv2.putText(hud, f"PERCLOS={feat.perclos:.2f}  BLINKS/min={feat.blinks}  YAWNS/min={feat.yawns}",
                    (10, 58), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2, cv2.LINE_AA)
    else:
        cv2.putText(hud, "SIN DATOS (sin rostro)", (10, 58), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50,200,255), 2, cv2.LINE_AA)

    cv2.putText(hud, f"FPS={fps:.1f}", (w-140,24), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200,200,200), 2, cv2.LINE_AA)
    if face_recent_s < 2.0:
        cv2.putText(hud, "Rostro OK", (w-140,58), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (144,238,144), 2, cv2.LINE_AA)
    else:
        cv2.putText(hud, "Sin rostro", (w-150,58), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2, cv2.LINE_AA)
    return hud

# ---------------- Main (UI en hilo principal) ----------------
def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--client_id", default=CLIENT_ID)
    ap.add_argument("--endpoint", default=ENDPOINT)
    ap.add_argument("--port", type=int, default=PORT)
    ap.add_argument("--ui", action="store_true", help="Mostrar ventana (imshow) si hay display")
    ap.add_argument("--headless", action="store_true", help="Forzar sin UI")
    args=ap.parse_args()

    # MQTT
    client = init_mqtt_aws(args.endpoint, args.port, args.client_id)

    # Threads worker
    t0 = T0Capture(); t0.start()
    t2 = T2FaceMesh(); t2.start()
    t3 = T3Features(); t3.start()
    t4 = T4Decision(args.client_id, client, TOPIC); t4.start()

    use_ui = (args.ui or os.getenv("DISPLAY")) and not args.headless
    if use_ui:
        cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(WINDOW_NAME, 820, 620)

    print("[MAIN] Corriendo. Ctrl+C para salir. Cámara: Picamera2 | FaceMesh: Tasks-only | UI:", bool(use_ui))
    try:
        while not stop.is_set():
            if use_ui:
                with shared.lock:
                    lf = shared.last_frame
                    feat = shared.last_feat
                    face_recent = now() - shared.face_ok_ts
                if lf is not None:
                    bgr, ts, fps = lf
                    if feat is not None:
                        blink_conf = (feat.blink_cov_s >= MIN_COVERAGE_BLINKS_S)
                        yawn_conf  = (feat.yawn_cov_s  >= MIN_COVERAGE_YAWNS_S)
                        est = classify_by_table(feat.perclos, feat.blinks, feat.yawns, blink_conf, yawn_conf)
                    else:
                        est = "SIN_DATOS"
                    hud = draw_hud(bgr.copy(), feat, est, face_recent, fps)
                    cv2.imshow(WINDOW_NAME, hud)
                    if (cv2.waitKey(1) & 0xFF) == ord('q'):
                        break
            time.sleep(0.01)
    except KeyboardInterrupt:
        pass
    finally:
        stop.set()
        for t in (t4,t3,t2,t0):
            t.join(timeout=1.0)
        if use_ui:
            try: cv2.destroyWindow(WINDOW_NAME)
            except: pass
        print("[MAIN] Bye.")

if __name__=="__main__":
    main()
