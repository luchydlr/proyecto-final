#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fatigue Edge RPi — Simple & Robust (Picamera2-first) + Reglas exactas + MQTT
- Cámara: intenta Picamera2 (libcamera). Si falla, cae a OpenCV V4L2.
- MediaPipe FaceMesh sobre frame completo (sin ROI por defecto).
- Métricas EXACTAS:
    perclos: fracción [0..1] redondeada a 2
    blinks : eventos/min (entero)
    yawns  : eventos/min (entero)
- Clasificación EXACTA por tabla, con salvaguardas:
    * No usa "blinks bajos" para somnolencia hasta tener >=45 s de cobertura
    * MICROSUEÑO por blinks<6 sólo si cobertura >=45 s; por PERCLOS>0.8 siempre
- Publica 1 vez por segundo (anti-spam).

Payload MQTT EXACTO:
{"device_id": "...", "estado": "...", "perclos": 0.xx, "blinks": int, "yawns": int, "ts": epoch}
"""

import time, json, threading, queue, argparse
from collections import deque
from dataclasses import dataclass
from typing import Optional, Tuple
import numpy as np
import cv2

# ====== MQTT opcional ======
try:
    from paho.mqtt import client as mqtt
except Exception:
    mqtt = None

# ====== Picamera2 opcional ======
PICAM2_OK = True
try:
    from picamera2 import Picamera2
except Exception:
    PICAM2_OK = False

# ====== MediaPipe FaceMesh ======
MP_OK = True
try:
    import mediapipe as mp
except Exception:
    MP_OK = False

# ---------------- Config (AQUÍ AJUSTAS) ----------------
CAM_SIZE = (640, 480)         # resolución de captura
TARGET_FPS = 30               # objetivo de FPS
FACEMESH_HZ = 20.0            # ~15–25 Hz va bien en RPi
EAR_BLINK_TH = 0.09
MAR_YAWN_TH  = 0.65

PERCLOS_WINDOW_SEC  = 10.0    # ventana para PERCLOS
EVENT_RATE_WINDOW_S = 60.0    # ventana para blinks/yawns por minuto

MIN_COVERAGE_BLINKS_S = 45.0  # exigir >=45 s para usar blinks en la decisión
MIN_COVERAGE_YAWNS_S  = 30.0  # exigir >=30 s para usar yawns en reglas estrictas

PUBLISH_HZ = 1.0              # publica 1 vez por segundo

# ---------------- Data ----------------
def now() -> float: return time.time()

@dataclass
class Frame: ts:float; bgr:np.ndarray; fps:float

@dataclass
class Landmarks: ts:float; pts:np.ndarray  # (468,2)

@dataclass
class Features:
    ts:float; ear:float; mar:float; perclos:float
    blinks:int; yawns:int
    blink_cov_s:float; yawn_cov_s:float

# ---------------- Queues ----------------
stop = threading.Event()
q_frame = queue.Queue(maxsize=4)
q_lm    = queue.Queue(maxsize=2)
q_feat  = queue.Queue(maxsize=4)

def put_drop(q:queue.Queue, item):
    try: q.put(item, timeout=0.003)
    except queue.Full:
        try: q.get_nowait()
        except Exception: pass
        q.put(item)

def get_latest(q:queue.Queue):
    last = None
    try:
        while True: last = q.get_nowait()
    except queue.Empty: pass
    return last

# ---------------- MediaPipe FaceMesh wrapper ----------------
class MPFaceMesh:
    def __init__(self):
        self.ok = MP_OK
        if self.ok:
            self.mesh = mp.solutions.face_mesh.FaceMesh(
                static_image_mode=False, max_num_faces=1, refine_landmarks=True,
                min_detection_confidence=0.5, min_tracking_confidence=0.5)
        else:
            self.mesh=None; print("[ERR] MediaPipe no disponible (instala 'mediapipe').")
    def __call__(self, bgr: np.ndarray) -> Optional[np.ndarray]:
        if not self.ok: return None
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
        r = self.mesh.process(rgb)
        if not r.multi_face_landmarks: return None
        lm = r.multi_face_landmarks[0].landmark
        h, w = bgr.shape[:2]
        pts = np.array([(p.x*w, p.y*h) for p in lm], dtype=np.float32)
        return pts

# ---------------- EAR / MAR ----------------
LE = [33,160,158,133,153,144]
RE = [263,387,385,362,380,373]
MOUTH_H = (13,14); MOUTH_W = (78,308)

def _ear6(p):
    d = lambda a,b: np.linalg.norm(p[a]-p[b])
    num = d(1,5)+d(2,4); den = 2.0*d(0,3)
    return (num/den) if den>1e-6 else 0.0
def compute_EAR(pts): return 0.5*(_ear6(pts[LE])+_ear6(pts[RE]))
def compute_MAR(pts):
    a,b = MOUTH_H; c,d = MOUTH_W
    h = np.linalg.norm(pts[a]-pts[b]); w = np.linalg.norm(pts[c]-pts[d])
    return (h/w) if w>1e-6 else 0.0

# ---------------- Filters / Windows ----------------
class EMA:
    def __init__(self, a=0.25): self.a=a; self.y=None
    def __call__(self,x): self.y = x if self.y is None else self.a*x+(1-self.a)*self.y; return self.y

class PERCLOSWindow:
    def __init__(self, seconds, hz): self.buf=deque(maxlen=int(seconds*hz))
    def update(self, eye_closed:bool)->float:
        self.buf.append(1 if eye_closed else 0)
        return sum(self.buf)/len(self.buf) if self.buf else 0.0

class EventRateWin:
    """Eventos/min en últimos win_s + cobertura (segundos de ventana poblada)."""
    def __init__(self, win_s=60.0):
        self.win=win_s; self.ts=deque(); self.first=None
    def push(self,t):
        self.ts.append(t); self.first = self.first or t
        self._trim(t)
    def _trim(self,tn):
        while self.ts and (tn-self.ts[0]>self.win): self.ts.popleft()
        if self.first and (tn - self.first > self.win):
            self.first = tn - self.win
    def rate_and_cov(self, tn):
        self._trim(tn)
        rate = len(self.ts)                 # eventos en últimos win_s
        cov  = 0.0 if self.first is None else max(0.0, min(self.win, tn - self.first))
        return rate, cov

class BlinkDetector:
    def __init__(self, th=EAR_BLINK_TH, min_open_frames=2):
        self.th=th; self.min_open=min_open_frames; self.closed=False; self.open_frames=0; self.rate=EventRateWin(EVENT_RATE_WINDOW_S)
    def update(self, ear_s, t):
        if ear_s<self.th: self.closed=True; self.open_frames=0
        else:
            if self.closed and self.open_frames>=self.min_open: self.rate.push(t)
            self.closed=False; self.open_frames+=1
    def measures(self,t): return self.rate.rate_and_cov(t)

class YawnDetector:
    def __init__(self, th=MAR_YAWN_TH, min_len_frames=5):
        self.th=th; self.min_len=min_len_frames; self.opening=0; self.rate=EventRateWin(EVENT_RATE_WINDOW_S)
    def update(self, mar_s, t):
        if mar_s>self.th: self.opening+=1
        else:
            if self.opening>=self.min_len: self.rate.push(t)
            self.opening=0
    def measures(self,t): return self.rate.rate_and_cov(t)

# ---------------- Clasificador exacto + confidencias ----------------
def closeness_to_range(x,a,b):
    if a<=x<=b: return 1.0
    d=min(abs(x-a),abs(x-b)); return max(0.0, 1.0 - d/max(1.0,(b-a)))

def classify_by_table(perclos, blinks, yawns, blink_conf, yawn_conf)->str:
    # Micorsueño por PERCLOS alto: siempre
    if perclos > 0.8:
        return "MICROSUEÑO"
    # Microsueño por blinks muy bajos: sólo si hay cobertura
    if blink_conf and blinks < 6:
        return "MICROSUEÑO"

    somno_blinks_ok = (6 <= blinks <= 12) if blink_conf else True
    somno_yawns_ok  = (yawns > 4) if yawn_conf else True
    if 0.4 <= perclos <= 0.8 and somno_blinks_ok and somno_yawns_ok:
        return "SOMNOLENCIA"

    fatiga_blinks_ok = (12 <= blinks <= 16) if blink_conf else True
    fatiga_yawns_ok  = (1 <= yawns <= 4) if yawn_conf else True
    if 0.3 <= perclos < 0.4 and fatiga_blinks_ok and fatiga_yawns_ok:
        return "FATIGA"

    normal_blinks_ok = (17 <= blinks <= 25) if blink_conf else True
    normal_yawns_ok  = (0 <= yawns <= 1) if yawn_conf else True
    if perclos < 0.3 and normal_blinks_ok and normal_yawns_ok:
        return "NORMAL"

    # Fallback por cercanía
    scores={
        "NORMAL"      : (0.3-perclos if perclos<0.3 else 0)+ (closeness_to_range(blinks,17,25) if blink_conf else 0.5) + (closeness_to_range(yawns,0,1) if yawn_conf else 0.5),
        "FATIGA"      : closeness_to_range(perclos,0.3,0.4) + (closeness_to_range(blinks,12,16) if blink_conf else 0.5) + (closeness_to_range(yawns,1,4) if yawn_conf else 0.5),
        "SOMNOLENCIA" : closeness_to_range(perclos,0.4,0.8) + (closeness_to_range(blinks,6,12)  if blink_conf else 0.5) + ((1.0 if yawn_conf and yawns>4 else 0.5))
    }
    if perclos>0.75:
        return "SOMNOLENCIA"
    return max(scores,key=scores.get)

# ---------------- MQTT ----------------
def init_mqtt(host, port, client_id):
    if mqtt is None:
        print("[MQTT] paho no disponible — modo consola"); return None
    c = mqtt.Client(client_id=client_id)
    try: c.connect(host,port,60); c.loop_start(); print("[MQTT] conectado"); return c
    except Exception as e: print("[MQTT] fallo:",e); return None

def mqtt_publish(client, topic, payload)->bool:
    if client is None:
        print(f"[MQTT-FAKE] {topic}: {json.dumps(payload)}"); return True
    try:
        r = client.publish(topic, json.dumps(payload), qos=1); r.wait_for_publish(timeout=1.0)
        return r.is_published()
    except Exception: return False

# ---------------- Camera backends ----------------
class CamBase:
    def start(self): ...
    def read(self): ...
    def stop(self): ...

class CamPicam2(CamBase):
    def __init__(self, size, fps):
        self.size=size; self.fps=fps; self.cam=None; self.last=now(); self._fps=0.0
    def start(self):
        self.cam = Picamera2()
        cfg = self.cam.create_video_configuration(main={"size": self.size, "format": "RGB888"})
        self.cam.configure(cfg); self.cam.start()
    def read(self):
        rgb = self.cam.capture_array()  # RGB
        bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
        t = now(); self._fps = 1.0/max(1e-3, t-self.last); self.last=t
        return True, bgr, self._fps
    def stop(self):
        try: self.cam.stop()
        except Exception: pass

class CamV4L2(CamBase):
    def __init__(self, size, fps):
        self.size=size; self.fps=fps; self.cap=None; self.last=now(); self._fps=0.0
    def start(self):
        self.cap = cv2.VideoCapture(0, cv2.CAP_V4L2)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.size[0])
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT,self.size[1])
        self.cap.set(cv2.CAP_PROP_FPS, self.fps)
        if not self.cap.isOpened():
            raise RuntimeError("No se pudo abrir /dev/video0 con V4L2")
    def read(self):
        ok,bgr = self.cap.read()
        if not ok: return False, None, 0.0
        t = now(); self._fps = 1.0/max(1e-3, t-self.last); self.last=t
        return True, bgr, self._fps
    def stop(self):
        try: self.cap.release()
        except Exception: pass

# ---------------- Threads ----------------
class T0Capture(threading.Thread):
    def __init__(self, backend="auto"):
        super().__init__(daemon=True)
        self.backend=backend; self.cam=None; self.fps=0.0
    def run(self):
        # selección de backend
        if self.backend=="picam2" or (self.backend=="auto" and PICAM2_OK):
            try:
                self.cam = CamPicam2(CAM_SIZE, TARGET_FPS); self.cam.start()
                print("[CAM] Picamera2 activo"); backend_ok=True
            except Exception as e:
                print("[CAM] Picamera2 falló:", e); backend_ok=False
        else:
            backend_ok=False

        if not backend_ok:
            self.cam = CamV4L2(CAM_SIZE, TARGET_FPS); self.cam.start()
            print("[CAM] V4L2 activo")

        while not stop.is_set():
            ok, bgr, fps = self.cam.read()
            if not ok: continue
            self.fps=fps
            put_drop(q_frame, Frame(ts=now(), bgr=bgr, fps=fps))
        self.cam.stop()

    def get_fps(self): return self.fps

class T2FaceMesh(threading.Thread):
    def __init__(self):
        super().__init__(daemon=True); self.fm=MPFaceMesh()
        self.period=1.0/FACEMESH_HZ; self.last=0.0
    def run(self):
        while not stop.is_set():
            fr = get_latest(q_frame) or q_frame.get()
            if (now()-self.last) < self.period:
                time.sleep(0.001); continue
            self.last=now()
            pts = self.fm(fr.bgr)   # full-frame (sin ROI por defecto)
            if pts is not None:
                put_drop(q_lm, Landmarks(ts=fr.ts, pts=pts))

class T3Features(threading.Thread):
    def __init__(self):
        super().__init__(daemon=True)
        self.ema_e, self.ema_m = EMA(0.25), EMA(0.25)
        self.perclos = PERCLOSWindow(PERCLOS_WINDOW_SEC, FACEMESH_HZ)
        self.blinks  = BlinkDetector(EAR_BLINK_TH,2)
        self.yawns   = YawnDetector(MAR_YAWN_TH,5)
    def run(self):
        while not stop.is_set():
            lm = q_lm.get()
            ear = compute_EAR(lm.pts); mar=compute_MAR(lm.pts)
            ear_s, mar_s = self.ema_e(ear), self.ema_m(mar)
            p = self.perclos.update(eye_closed=(ear_s<EAR_BLINK_TH))
            self.blinks.update(ear_s, lm.ts)
            self.yawns.update(mar_s, lm.ts)
            bl, bl_cov = self.blinks.measures(lm.ts)
            yw, yw_cov = self.yawns.measures(lm.ts)
            put_drop(q_feat, Features(ts=lm.ts, ear=ear_s, mar=mar_s, perclos=p,
                                      blinks=bl, yawns=yw,
                                      blink_cov_s=bl_cov, yawn_cov_s=yw_cov))

class T4Decision(threading.Thread):
    def __init__(self, client_id, mqtt_client, topic="fatiga/estado"):
        super().__init__(daemon=True)
        self.did = client_id; self.mqtt=mqtt_client; self.topic=topic
        self.last_pub = 0.0
    def run(self):
        while not stop.is_set():
            f = q_feat.get()
            blink_conf = (f.blink_cov_s >= MIN_COVERAGE_BLINKS_S)
            yawn_conf  = (f.yawn_cov_s  >= MIN_COVERAGE_YAWNS_S)
            estado = classify_by_table(f.perclos, f.blinks, f.yawns, blink_conf, yawn_conf)

            # Publicación a 1 Hz
            tnow = now()
            if (tnow - self.last_pub) >= (1.0 / PUBLISH_HZ):
                payload = {
                    "device_id": self.did,
                    "estado": estado,
                    "perclos": round(float(f.perclos), 2),
                    "blinks": int(f.blinks),   # eventos/min últimos 60 s (entero)
                    "yawns": int(f.yawns),     # eventos/min últimos 60 s (entero)
                    "ts": int(f.ts)
                }
                mqtt_publish(self.mqtt, self.topic, payload)
                self.last_pub = tnow

# ---------------- Main ----------------
def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--client_id", default="RPI 5")
    ap.add_argument("--mqtt_host", default="localhost")
    ap.add_argument("--mqtt_port", type=int, default=1883)
    ap.add_argument("--camera", choices=["auto","picam2","v4l2"], default="auto",
                    help="Preferir Picamera2 (libcamera) o V4L2")
    ap.add_argument("--use_roi", action="store_true",
                    help="(Opcional) usar Face Detection para ROI (NO recomendado si apenas pruebas)")
    args=ap.parse_args()

    # MQTT
    client = init_mqtt(args.mqtt_host, args.mqtt_port, args.client_id)

    # Threads
    t0 = T0Capture(backend=args.camera)
    t2 = T2FaceMesh()
    t3 = T3Features()
    t4 = T4Decision(args.client_id, client, "fatiga/estado")

    for t in (t0,t2,t3,t4): t.start()
    print("[MAIN] Corriendo. Ctrl+C para salir. Cámara:", args.camera, "| ROI:", args.use_roi)
    try:
        while not stop.is_set(): time.sleep(0.05)
    except KeyboardInterrupt:
        pass
    finally:
        stop.set()
        for t in (t4,t3,t2,t0):
            t.join(timeout=1.0)
        print("[MAIN] Bye.")

if __name__=="__main__":
    main()
